{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hdbcli --break-system-packages\n",
    "%pip install generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install folium --break-system-packages\n",
    "%pip install ipywidgets --break-system-packages\n",
    "\n",
    "# kernel restart required!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AICORE_AUTH_URL'] = 'xxx'\n",
    "os.environ['AICORE_CLIENT_ID'] = 'xxx'\n",
    "os.environ['AICORE_CLIENT_SECRET'] = 'xxx'\n",
    "os.environ['AICORE_RESOURCE_GROUP'] = 'default'\n",
    "os.environ['AICORE_BASE_URL'] = 'https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com'\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from hdbcli import dbapi\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema using Pydantic v2\n",
    "class OutputSchema(BaseModel):\n",
    "    commodity_code: str = Field(description=\"The code representing the commodity\")\n",
    "    keywords: list[str] = Field(description=\"A list of keywords related to the commodity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(proxy_model_name='text-embedding-ada-002')\n",
    "\n",
    "template = \"\"\" Objective: Create semantic embeddings to accurately map invoice line items to UNSPSC codes using keyword variations, \n",
    "based on the provided UNSPSC description.\n",
    "\n",
    "Guidelines:  \n",
    "1. UNSPSC Structure Analysis\n",
    "- Analyze hierarchy: Segment > Family > Class > Commodity  \n",
    "- Focus on commodity-level descriptions (most granular tier).  \n",
    "\n",
    "2. Keyword Extraction\n",
    "- Primary Term: Identify the product name or type (e.g., \"tomatoes\", \"pasta\").\n",
    "- Secondary Term: Specify the product's state or packaging (e.g., \"canned\", \"frozen\", \"glass jars\").\n",
    "- Generalization: Standardize specific terms to broader categories (e.g., \"ginger thins → biscuit\", \"liquorice → candy\").\n",
    "- Exclusion: Remove non-essential words such as articles, generic verbs, or irrelevant descriptors.\n",
    "\n",
    "3. Embedding Generation Rules\n",
    "- Create 3 variants per code:  \n",
    "  - Line 1: Original UNSPSC description keywords  \n",
    "  - Lines 2-3: Semantic equivalents preserving core meaning  \n",
    "- Avoid terms overlapping with other UNSPSC entries.  \n",
    "\n",
    "Output Format: {format_instructions}\n",
    "\n",
    "UNSPSC description: {UNSPSC}\n",
    "\"\"\"\n",
    "\n",
    "#prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the JSON output parser with Pydantic v2 schema\n",
    "parser = JsonOutputParser(pydantic_object=OutputSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to query the LLM with vector-based context\n",
    "def retrieve_and_query_llm(UNSPSC: str, k: int = 1) -> str:\n",
    "\n",
    "    #prompt = prompt_template.format(UNSPSC=UNSPSC)\n",
    "    # Query the LLM\n",
    "    proxy_client = get_proxy_client('gen-ai-hub')\n",
    "    model = ChatOpenAI(proxy_model_name='gpt-4o', proxy_client=proxy_client)\n",
    "\n",
    "    # Create prompt template with format instructions\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"UNSPSC\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Create and run the chain\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    # Example usage\n",
    "    result = chain.invoke({\"UNSPSC\": UNSPSC})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/'\n",
    "file_in_name = 'Maersk_UNSPSC_50170000.csv'\n",
    "file_out_name = 'Maersk_UNSPSC_5017000_Enriched.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(file_path + file_in_name, mode ='r') as infile, open(file_path + file_out_name, mode='w', encoding='utf-8', newline='\\n') as outfile:\n",
    "    reader = csv.reader(infile, delimiter=';')\n",
    "    writer = csv.writer(outfile, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    for row_count, inline in enumerate(reader):\n",
    "        if (row_count == 0):\n",
    "            outline = ['Commodity Code; Commodity Keywords; Examples']\n",
    "            cleaned_data = outline[0].strip(\"'\").strip('\"')\n",
    "            cleaned_data = cleaned_data.split('\\n')\n",
    "            #print(cleaned_data)\n",
    "            for row in cleaned_data:\n",
    "                    #print('Row: ', row)\n",
    "                    writer.writerow(row.split(';'))\n",
    "        \n",
    "        else:\n",
    "            if (len(inline[11]) > 0):\n",
    "                try:\n",
    "                    commodity_code = int(inline[11])\n",
    "\n",
    "                    if (commodity_code < 50199000):\n",
    "                        print('Row Count: ', row_count, commodity_code)\n",
    "                        response = retrieve_and_query_llm(str(inline))\n",
    "                        #print('inline: ', inline, ' - ', response)\n",
    "                        #print(response['commodity_code'], response['keywords'], type(response['keywords']))\n",
    "\n",
    "                        for keyword in response['keywords']:\n",
    "                            outline = str(response['commodity_code']) + ';' + str(keyword)\n",
    "                            writer.writerow(outline.split(';'))\n",
    "\n",
    "                except:\n",
    "                    print('error')\n",
    "\n",
    "        if (row_count == 10):\n",
    "            break\n",
    "\n",
    "print('FINISHED')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "\n",
    "with open(file_path + file_out_name, mode ='r') as infile:\n",
    "    reader = csv.reader(infile, delimiter=';')\n",
    "    for row_count, inline in enumerate(reader):\n",
    "        if row_count > 0:\n",
    "            print(inline)\n",
    "            commodity_code = inline[0]\n",
    "            commodity_keywords = inline[1]\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=commodity_keywords,\n",
    "                    metadata={\n",
    "                        \"item_number\": commodity_code\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        #if row_count == 10:\n",
    "        #    break\n",
    "\n",
    "#print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = dbapi.connect(\n",
    "    address='xxx',\n",
    "    port='443',\n",
    "    user='xxx',\n",
    "    password='xxx',\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")\n",
    "\n",
    "db = HanaDB(\n",
    "    embedding=embeddings,\n",
    "    connection=connection,\n",
    "    table_name=\"Maersk_Embeddings\"\n",
    ")\n",
    "\n",
    "# Delete already existing documents from the table\n",
    "db.delete(filter={})\n",
    "\n",
    "#db.initialize()\n",
    "db.add_documents(documents)\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = 'Maersk_InvoiceLines.csv'\n",
    "\n",
    "# Read the CSV file with UTF-8 encoding\n",
    "df = pd.read_csv(file_path + file_name, delimiter=';', encoding='utf-8')\n",
    "\n",
    "print(df)\n",
    "\n",
    "#df = df.sort_values(by=['DOCID', 'ITEMID'])\n",
    "#df = df.drop_duplicates(subset=['SGTXT'], ignore_index = True)\n",
    "print(df.shape[0])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    Invoice_Line = row['INVOICE_LINE']\n",
    "    UNSPSC_predict_code = int(row['UNSPSC prediction'])\n",
    "    UNSPSC_predict_label = row['UNSPSC prediction label']\n",
    "    UNSPSC_predict_similarity = row['UNSPSC prediction similarity']\n",
    "\n",
    "    print(index, Invoice_Line, ' - ', UNSPSC_predict_code, UNSPSC_predict_label, UNSPSC_predict_similarity) \n",
    "    similar_items = db.similarity_search_with_score(Invoice_Line, k=3)\n",
    "\n",
    "    for item in similar_items:\n",
    "        #print(item)\n",
    "        commodity_code = item[0].metadata['item_number']\n",
    "        commodity_label = str(item[0].page_content).strip()\n",
    "        score = item[1]\n",
    "\n",
    "        if (UNSPSC_predict_code == commodity_code):\n",
    "            correct = correct+1\n",
    "\n",
    "        print(commodity_code, commodity_label, score)\n",
    "\n",
    "    print('Number of correct: ', correct)\n",
    "    print()\n",
    "\n",
    "    #if index == 100:\n",
    "    #   break\n",
    "\n",
    "print('Finished...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
